{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def scrape_karkidi_jobs(keyword=\"data science\", pages=1):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    base_url = \"https://www.karkidi.com/Find-Jobs/{page}/all/India?search={query}\"\n",
        "    jobs_list = []\n",
        "\n",
        "    for page in range(1, pages + 1):\n",
        "        url = base_url.format(page=page, query=keyword.replace(' ', '%20'))\n",
        "        print(f\"Scraping page: {page}\")\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        job_blocks = soup.find_all(\"div\", class_=\"ads-details\")\n",
        "        for job in job_blocks:\n",
        "            try:\n",
        "                title = job.find(\"h4\").get_text(strip=True)\n",
        "                company = job.find(\"a\", href=lambda x: x and \"Employer-Profile\" in x).get_text(strip=True)\n",
        "                location = job.find(\"p\").get_text(strip=True)\n",
        "                experience = job.find(\"p\", class_=\"emp-exp\").get_text(strip=True)\n",
        "                key_skills_tag = job.find(\"span\", string=\"Key Skills\")\n",
        "                skills = key_skills_tag.find_next(\"p\").get_text(strip=True) if key_skills_tag else \"\"\n",
        "                summary_tag = job.find(\"span\", string=\"Summary\")\n",
        "                summary = summary_tag.find_next(\"p\").get_text(strip=True) if summary_tag else \"\"\n",
        "\n",
        "                jobs_list.append({\n",
        "                    \"Title\": title,\n",
        "                    \"Company\": company,\n",
        "                    \"Location\": location,\n",
        "                    \"Experience\": experience,\n",
        "                    \"Summary\": summary,\n",
        "                    \"Skills\": skills\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing job block: {e}\")\n",
        "                continue\n",
        "\n",
        "        time.sleep(1)  # Be nice to the server\n",
        "\n",
        "    return pd.DataFrame(jobs_list)\n",
        "\n",
        "# Example use:\n",
        "if __name__ == \"__main__\":\n",
        "    df_jobs = scrape_karkidi_jobs(keyword=\"data science\", pages=2)\n",
        "    print(df_jobs.head())\n",
        "    # Add this line to save the DataFrame to a CSV file\n",
        "    df_jobs.to_csv(\"karkidi_jobs.csv\", index=False)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9gffDOU3fJG",
        "outputId": "96bda032-24c2-4cd0-c4ee-8de63ccef33e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page: 1\n",
            "Scraping page: 2\n",
            "                                               Title         Company  \\\n",
            "0          Machine Learning Physical Design Engineer          Google   \n",
            "1  Staff Software Engineer - Monetization, Poe (R...     Quora, Inc.   \n",
            "2  Staff Backend Engineer - Bot Creator Ecosystem...     Quora, Inc.   \n",
            "3  Senior Backend Engineer - Bot Creator Ecosyste...     Quora, Inc.   \n",
            "4                         Data Scientist Lead - AIML  JPMorgan Chase   \n",
            "\n",
            "                      Location Experience  \\\n",
            "0  Bengaluru, Karnataka, India   4-6 year   \n",
            "1                        India  8-10 year   \n",
            "2                        India  8-10 year   \n",
            "3                        India   6-8 year   \n",
            "4  Bengaluru, Karnataka, India   6-8 year   \n",
            "\n",
            "                                             Summary  \\\n",
            "0  Minimum qualifications:Bachelor's degree in El...   \n",
            "1  About Quora:Quora‚Äôs mission is to grow and sha...   \n",
            "2  About Quora:Quora‚Äôs mission is to grow and sha...   \n",
            "3  About Quora:Quora‚Äôs mission is to grow and sha...   \n",
            "4  We have an opportunity to impact your career a...   \n",
            "\n",
            "                                              Skills  \n",
            "0  Aartificial intelligence,Algorithms,Data struc...  \n",
            "1  Aartificial intelligence,Analytical and Proble...  \n",
            "2  Aartificial intelligence,API,Data science tech...  \n",
            "3  Aartificial intelligence,API,Data science tech...  \n",
            "4  Aartificial intelligence,Data science techniqu...  \n"
          ]
        }
      ]
    },
    {
      "source": [
        "# Convert skills to lowercase, strip spaces\n",
        "def clean_skills(skill_str):\n",
        "    if pd.isnull(skill_str):\n",
        "        return \"\"\n",
        "    # Ensure skill_str is a string before splitting\n",
        "    if isinstance(skill_str, str):\n",
        "        skills = [skill.strip().lower() for skill in skill_str.split(',')]\n",
        "        return ' '.join(skills)  # join with space for vectorization\n",
        "    else:\n",
        "        return \"\" # Handle non-string input gracefully\n",
        "\n",
        "# Use df_jobs instead of df\n",
        "df_jobs['cleaned_skills'] = df_jobs['Skills'].apply(clean_skills)\n",
        "\n",
        "# You can now display or save the updated DataFrame\n",
        "print(df_jobs.head())"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeqNK4br3sZZ",
        "outputId": "1673bc71-603c-45ef-e26b-44c4ae410432"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               Title         Company  \\\n",
            "0          Machine Learning Physical Design Engineer          Google   \n",
            "1  Staff Software Engineer - Monetization, Poe (R...     Quora, Inc.   \n",
            "2  Staff Backend Engineer - Bot Creator Ecosystem...     Quora, Inc.   \n",
            "3  Senior Backend Engineer - Bot Creator Ecosyste...     Quora, Inc.   \n",
            "4                         Data Scientist Lead - AIML  JPMorgan Chase   \n",
            "\n",
            "                      Location Experience  \\\n",
            "0  Bengaluru, Karnataka, India   4-6 year   \n",
            "1                        India  8-10 year   \n",
            "2                        India  8-10 year   \n",
            "3                        India   6-8 year   \n",
            "4  Bengaluru, Karnataka, India   6-8 year   \n",
            "\n",
            "                                             Summary  \\\n",
            "0  Minimum qualifications:Bachelor's degree in El...   \n",
            "1  About Quora:Quora‚Äôs mission is to grow and sha...   \n",
            "2  About Quora:Quora‚Äôs mission is to grow and sha...   \n",
            "3  About Quora:Quora‚Äôs mission is to grow and sha...   \n",
            "4  We have an opportunity to impact your career a...   \n",
            "\n",
            "                                              Skills  \\\n",
            "0  Aartificial intelligence,Algorithms,Data struc...   \n",
            "1  Aartificial intelligence,Analytical and Proble...   \n",
            "2  Aartificial intelligence,API,Data science tech...   \n",
            "3  Aartificial intelligence,API,Data science tech...   \n",
            "4  Aartificial intelligence,Data science techniqu...   \n",
            "\n",
            "                                      cleaned_skills  \n",
            "0  aartificial intelligence algorithms data struc...  \n",
            "1  aartificial intelligence analytical and proble...  \n",
            "2  aartificial intelligence api data science tech...  \n",
            "3  aartificial intelligence api data science tech...  \n",
            "4  aartificial intelligence data science techniqu...  \n"
          ]
        }
      ]
    },
    {
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Transform cleaned skill text into TF-IDF matrix\n",
        "# Use df_jobs instead of df\n",
        "X = vectorizer.fit_transform(df_jobs['cleaned_skills'])\n",
        "\n",
        "# Optional: view feature names (skills)\n",
        "print(vectorizer.get_feature_names_out())"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8_4Pxvy34FY",
        "outputId": "197efa59-22d0-4bb8-e83f-ff051527a7a4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aartificial' 'algorithms' 'amazon' 'analysis' 'analytical' 'and'\n",
            " 'apache' 'api' 'aws' 'azure' 'bigquery' 'cloud' 'communication' 'cycle'\n",
            " 'data' 'database' 'design' 'effective' 'employee' 'gcp' 'google'\n",
            " 'graphql' 'hadoop' 'intelligence' 'js' 'k8s' 'kubernetes' 'language'\n",
            " 'large' 'leadership' 'learning' 'life' 'llms' 'machine' 'mlops' 'models'\n",
            " 'natural' 'next' 'nlp' 'optimization' 'platform' 'problem' 'processing'\n",
            " 'programming' 'python' 'react' 'redshift' 'science' 'skill' 'skills'\n",
            " 'solving' 'sql' 'structuring' 'teamwork' 'techniques' 'tools'\n",
            " 'typescript']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save vectorized features and vectorizer for later reuse\n",
        "joblib.dump(vectorizer, 'skills_vectorizer.pkl')\n",
        "pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out()).to_csv(\"skills_tfidf_matrix.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "TexRMJDJ38Hy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Assume `X` is the TF-IDF matrix from step 2\n",
        "# Try different cluster numbers to find the best one\n",
        "best_k = 0\n",
        "best_score = -1\n",
        "\n",
        "for k in range(2, 11):  # Try cluster sizes from 2 to 10\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Assume `X` is the TF-IDF matrix from step 2\n",
        "# Try different cluster numbers to find the best one\n",
        "best_k = 0\n",
        "best_score = -1\n",
        "\n",
        "for k in range(2, 11):  # Try cluster sizes from 2 to 10\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    labels = kmeans.fit_predict(X)\n",
        "    score = silhouette_score(X, labels)\n",
        "    print(f\"k={k}, silhouette score={score}\")\n",
        "\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_k = k\n",
        "\n",
        "print(f\"Best k: {best_k} with silhouette score: {best_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ju6IhsRL4EJp",
        "outputId": "b8227a4a-70e7-435f-d708-de34735afa6c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k=2, silhouette score=0.2307726709693918\n",
            "k=3, silhouette score=0.3250572727174708\n",
            "k=4, silhouette score=0.4347476118666573\n",
            "k=5, silhouette score=0.5520180165735169\n",
            "k=6, silhouette score=0.6793479939977749\n",
            "k=7, silhouette score=0.8025064335380818\n",
            "k=8, silhouette score=0.9384450192702246\n",
            "k=9, silhouette score=1.0\n",
            "k=10, silhouette score=1.0\n",
            "Best k: 9 with silhouette score: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# Final KMeans model\n",
        "kmeans = KMeans(n_clusters=best_k, random_state=42)\n",
        "# Use df_jobs instead of df to add the cluster column\n",
        "df_jobs['cluster'] = kmeans.fit_predict(X)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "0vtxEIUV4Nqk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "source": [
        "# Check most common skills per cluster\n",
        "for i in range(best_k):\n",
        "    print(f\"\\n--- Cluster {i} ---\")\n",
        "    # Use df_jobs instead of df\n",
        "    cluster_jobs = df_jobs[df_jobs['cluster'] == i]\n",
        "    all_skills = ' '.join(cluster_jobs['cleaned_skills'])\n",
        "    from collections import Counter\n",
        "    skill_counts = Counter(all_skills.split())\n",
        "    top_skills = skill_counts.most_common(10)\n",
        "    for skill, count in top_skills:\n",
        "        print(f\"{skill}: {count}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nb8dD0tv4U7P",
        "outputId": "04aeb18d-895f-4934-cfa0-d1d40974eb0b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Cluster 0 ---\n",
            "language: 4\n",
            "aartificial: 2\n",
            "intelligence: 2\n",
            "large: 2\n",
            "models: 2\n",
            "-: 2\n",
            "llms: 2\n",
            "machine: 2\n",
            "learning: 2\n",
            "techniques: 2\n",
            "\n",
            "--- Cluster 1 ---\n",
            "aartificial: 4\n",
            "intelligence: 4\n",
            "api: 4\n",
            "data: 4\n",
            "science: 4\n",
            "techniques: 4\n",
            "design: 4\n",
            "effective: 4\n",
            "communication: 4\n",
            "skills: 4\n",
            "\n",
            "--- Cluster 2 ---\n",
            "aartificial: 2\n",
            "intelligence: 2\n",
            "aws: 2\n",
            "azure: 2\n",
            "google: 2\n",
            "cloud: 2\n",
            "platform: 2\n",
            "(gcp): 2\n",
            "kubernetes-k8s: 2\n",
            "large: 2\n",
            "\n",
            "--- Cluster 3 ---\n",
            "design: 2\n",
            "leadership: 2\n",
            "skill: 2\n",
            "machine: 2\n",
            "learning: 2\n",
            "techniques: 2\n",
            "\n",
            "--- Cluster 4 ---\n",
            "google: 4\n",
            "programming: 4\n",
            "amazon: 2\n",
            "redshift: 2\n",
            "apache: 2\n",
            "hadoop: 2\n",
            "data: 2\n",
            "science: 2\n",
            "techniques: 2\n",
            "bigquery: 2\n",
            "\n",
            "--- Cluster 5 ---\n",
            "aartificial: 2\n",
            "intelligence: 2\n",
            "algorithms: 2\n",
            "data: 2\n",
            "structuring: 2\n",
            "design: 2\n",
            "machine: 2\n",
            "learning: 2\n",
            "techniques: 2\n",
            "\n",
            "--- Cluster 6 ---\n",
            "algorithms: 2\n",
            "employee: 2\n",
            "life: 2\n",
            "cycle: 2\n",
            "kubernetes-k8s: 2\n",
            "large: 2\n",
            "language: 2\n",
            "models: 2\n",
            "-: 2\n",
            "llms: 2\n",
            "\n",
            "--- Cluster 7 ---\n",
            "aartificial: 2\n",
            "intelligence: 2\n",
            "analytical: 2\n",
            "and: 2\n",
            "problem: 2\n",
            "solving: 2\n",
            "api: 2\n",
            "data: 2\n",
            "analysis: 2\n",
            "database: 2\n",
            "\n",
            "--- Cluster 8 ---\n",
            "techniques: 4\n",
            "language: 4\n",
            "aartificial: 2\n",
            "intelligence: 2\n",
            "data: 2\n",
            "science: 2\n",
            "large: 2\n",
            "models: 2\n",
            "-: 2\n",
            "llms: 2\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import joblib\n",
        "\n",
        "# Save the clustering model\n",
        "joblib.dump(kmeans, 'karkidi_cluster_model.pkl')\n",
        "\n",
        "# Save the labeled data\n",
        "# Use df_jobs instead of df to save the correct DataFrame\n",
        "df_jobs.to_csv(\"karkidi_clustered_jobs.csv\", index=False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "_56uY_PR4coY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Load the saved TF-IDF vectorizer and KMeans clustering model\n",
        "vectorizer = joblib.load('skills_vectorizer.pkl')\n",
        "kmeans = joblib.load('karkidi_cluster_model.pkl')\n"
      ],
      "metadata": {
        "id": "JiBk_K604d39"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "source": [
        "# Reuse the cleaning function from earlier\n",
        "import pandas as pd # Import pandas if not already imported\n",
        "\n",
        "def clean_skills(skill_str):\n",
        "    if pd.isnull(skill_str):\n",
        "        return \"\"\n",
        "    # Ensure skill_str is a string before splitting\n",
        "    if isinstance(skill_str, str):\n",
        "        skills = [skill.strip().lower() for skill in skill_str.split(',')]\n",
        "        return ' '.join(skills)\n",
        "    else:\n",
        "        return \"\" # Handle non-string input gracefully\n",
        "\n",
        "\n",
        "# --- Add the following code to create a sample new_jobs_df ---\n",
        "# In a real application, you would load your new data here\n",
        "data = {'skills': ['python, machine learning, data science', 'java, spring, hibernate', 'data analysis, sql, excel']}\n",
        "new_jobs_df = pd.DataFrame(data)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "\n",
        "new_jobs_df['cleaned_skills'] = new_jobs_df['skills'].apply(clean_skills)\n",
        "\n",
        "# Convert new job skills into TF-IDF vectors\n",
        "X_new = vectorizer.transform(new_jobs_df['cleaned_skills'])\n",
        "\n",
        "# Predict clusters for new jobs\n",
        "new_jobs_df['cluster'] = kmeans.predict(X_new)\n",
        "\n",
        "# Optional: Display the new DataFrame with cluster assignments\n",
        "print(new_jobs_df)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGEQD4Qe4piY",
        "outputId": "4d9ca358-2657-403c-b20a-73764571f101"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                   skills  \\\n",
            "0  python, machine learning, data science   \n",
            "1                 java, spring, hibernate   \n",
            "2               data analysis, sql, excel   \n",
            "\n",
            "                         cleaned_skills  cluster  \n",
            "0  python machine learning data science        8  \n",
            "1                 java spring hibernate        1  \n",
            "2               data analysis sql excel        4  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save for reference or further processing\n",
        "new_jobs_df.to_csv(\"new_classified_jobs.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "NgfgL4Op4q7F"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install schedule"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYyJWe6W4y6r",
        "outputId": "591102ca-6f44-4f12-a8d5-524dfdb3c4c8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting schedule\n",
            "  Downloading schedule-1.2.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "Downloading schedule-1.2.2-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: schedule\n",
            "Successfully installed schedule-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_preferences = {\n",
        "    \"alice@example.com\": [0, 2],\n",
        "    \"bob@example.com\": [1],\n",
        "}\n"
      ],
      "metadata": {
        "id": "Bm7Hb_ET5y4C"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "source": [
        "# Reuse the cleaning function from earlier\n",
        "import pandas as pd # Import pandas if not already imported\n",
        "\n",
        "def clean_skills(skill_str):\n",
        "    if pd.isnull(skill_str):\n",
        "        return \"\"\n",
        "    # Ensure skill_str is a string before splitting\n",
        "    if isinstance(skill_str, str):\n",
        "        skills = [skill.strip().lower() for skill in skill_str.split(',')]\n",
        "        return ' '.join(skills)\n",
        "    else:\n",
        "        return \"\" # Handle non-string input gracefully\n",
        "\n",
        "\n",
        "# --- Add the following code to create a sample new_jobs_df ---\n",
        "# In a real application, you would load your new data here from the scraper\n",
        "# For demonstration, let's create a sample DataFrame that mimics the structure\n",
        "# of the scraped data, including 'Title', 'Company', and 'Skills'.\n",
        "data = {\n",
        "    'Title': ['Data Scientist', 'Java Developer', 'Data Analyst'],\n",
        "    'Company': ['Tech Solutions', 'Code Masters', 'Data Insights'],\n",
        "    'Skills': ['python, machine learning, data science', 'java, spring, hibernate', 'data analysis, sql, excel']\n",
        "}\n",
        "new_jobs_df = pd.DataFrame(data)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "\n",
        "new_jobs_df['cleaned_skills'] = new_jobs_df['Skills'].apply(clean_skills)\n",
        "\n",
        "# Convert new job skills into TF-IDF vectors\n",
        "X_new = vectorizer.transform(new_jobs_df['cleaned_skills'])\n",
        "\n",
        "# Predict clusters for new jobs\n",
        "new_jobs_df['cluster'] = kmeans.predict(X_new)\n",
        "\n",
        "# Optional: Display the new DataFrame with cluster assignments\n",
        "print(new_jobs_df)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lU0QcjC6H5L",
        "outputId": "df131d86-0d53-40a7-e850-a701898e46cf"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Title         Company                                  Skills  \\\n",
            "0  Data Scientist  Tech Solutions  python, machine learning, data science   \n",
            "1  Java Developer    Code Masters                 java, spring, hibernate   \n",
            "2    Data Analyst   Data Insights               data analysis, sql, excel   \n",
            "\n",
            "                         cleaned_skills  cluster  \n",
            "0  python machine learning data science        8  \n",
            "1                 java spring hibernate        1  \n",
            "2               data analysis sql excel        4  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hffkt6Yk7fAi",
        "outputId": "95b9c7af-9634-45bc-cc33-66f417411798"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.45.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.39.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.45.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.45.1 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üõ† Create the app.py file inside Colab\n",
        "\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load models\n",
        "vectorizer = joblib.load(\"skills_vectorizer.pkl\")\n",
        "kmeans = joblib.load(\"karkidi_cluster_model.pkl\")\n",
        "\n",
        "# Load clustered job data\n",
        "job_data = pd.read_csv(\"karkidi_clustered_jobs.csv\")\n",
        "\n",
        "# Title\n",
        "st.set_page_config(page_title=\"Karkidi Job Recommender\", layout=\"wide\")\n",
        "st.title(\"üîç Job Recommender Based on Your Skills\")\n",
        "\n",
        "# Login Simulation via GitHub (done automatically via Streamlit sharing)\n",
        "st.markdown(\"**Logged in via GitHub (Streamlit Sharing handles this automatically).**\")\n",
        "\n",
        "# User skill input\n",
        "user_input = st.text_input(\"Enter your skills (comma separated):\", \"python, machine learning, sql\")\n",
        "\n",
        "if st.button(\"Find Matching Jobs\"):\n",
        "    def clean_skills(skill_str):\n",
        "        skills = [skill.strip().lower() for skill in skill_str.split(',')]\n",
        "        return ' '.join(skills)\n",
        "\n",
        "    cleaned = clean_skills(user_input)\n",
        "    user_vector = vectorizer.transform([cleaned])\n",
        "    user_cluster = kmeans.predict(user_vector)[0]\n",
        "\n",
        "    st.success(f\"üìå Based on your skills, you match **Cluster {user_cluster}**.\")\n",
        "\n",
        "    matching_jobs = job_data[job_data['cluster'] == user_cluster]\n",
        "    st.subheader(f\"üìÑ Found {len(matching_jobs)} matching jobs:\")\n",
        "\n",
        "    for _, row in matching_jobs.iterrows():\n",
        "        st.markdown(f\"\"\"\n",
        "        ---\n",
        "        ### {row['Title']}\n",
        "        - **Company**: {row['Company']}\n",
        "        - **Location**: {row['Location']}\n",
        "        - **Experience**: {row['Experience']}\n",
        "        - **Skills**: {row['Skills']}\n",
        "        - **Summary**: {row['Summary']}\n",
        "        \"\"\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dy9aJrYd7h2c",
        "outputId": "998387d1-93a9-4455-ab9e-0df0e7a4ccd9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    }
  ]
}